{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfG2LGqkrZK5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow import nn\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import deserialize, serialize\n",
        "from tensorflow.python.keras.saving import saving_utils\n",
        "\n",
        "\n",
        "from copy import deepcopy\n",
        "import dask\n",
        "from dask import delayed\n",
        "from dask.distributed import Client\n",
        "import pickle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save/Load Settings\n",
        "load_model = None  # None to train a new model; filename to load and plot an old model\n",
        "\n",
        "# RL Hyperparameters\n",
        "LAMBDA_actor = 0.8\n",
        "LAMBDA_critic = 0.8\n",
        "lr_actor = 0.001\n",
        "lr_critic = 0.001\n",
        "P = 1\n",
        "Xt_sigma_base = tf.Variable([1.0])\n",
        "Xt_sigma_scale = tf.Variable([0.0])\n",
        "\n",
        "# Iterations\n",
        "MAX_ITER = 20000\n",
        "BATCH_SIZE = 1\n",
        "N_models = 1\n",
        "N_simulations = 100\n",
        "\n",
        "# Parallel settings\n",
        "dask_scheduler = 'single-threaded'\n",
        "N_workers = N_models\n",
        "if dask_scheduler == 'local':\n",
        "    client = Client(num_workers=N_workers)\n",
        "elif dask_scheduler not in ['single-threaded', 'processes', 'threads']:\n",
        "    client = Client(dask_scheduler)\n",
        "\n",
        "# Diagnostic output settings\n",
        "print_window = 100\n",
        "movavg_window = 1000\n",
        "window_startfrom = 100\n",
        "verbose = 1  # (None = no printing; 1 = minimal printing; 2 = full printing)\n",
        "\n",
        "# Model Parameters\n",
        "S = 2\n",
        "K = 2\n",
        "gamma = 0.5\n",
        "rho = 0.05\n",
        "rf = 1/(1-rho)-1\n",
        "R = 0.1\n",
        "phi = tf.Variable([[0.1, 0], [0, 0.4]])\n",
        "sigma = tf.eye(S)\n",
        "omega = tf.eye(S)\n",
        "B = tf.eye(S)\n",
        "w = 0.5\n",
        "Atilde = 2*tf.eye(S)\n",
        "A = w*Atilde\n",
        "C = (1-w)*Atilde\n",
        "X0 = tf.zeros([S,1])\n",
        "f0 = tf.ones([S,1])\n",
        "D0 = tf.zeros([S,1])"
      ],
      "metadata": {
        "id": "CRRpwafxrqYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class stockEnv():\n",
        "    \"\"\"\n",
        "    An environment for agents to trade on assets\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Pass in the dataset to self.df\n",
        "        '''\n",
        "        self.t = 0\n",
        "        self.X_last = X0\n",
        "        self.ft = f0\n",
        "        self.Dt = D0\n",
        "\n",
        "    def reset(self):\n",
        "        '''\n",
        "        Reset env for new episode and return H0\n",
        "        '''\n",
        "        self.t = 0\n",
        "        self.X_last = X0\n",
        "        self.ft = f0\n",
        "        self.Dt = D0\n",
        "        return self.X_last, self.ft, self.Dt\n",
        "\n",
        "    def step(self, Xt, random=True):\n",
        "        X_last = self.X_last\n",
        "        self.X_last = Xt\n",
        "        TR = TC(Xt, X_last) + (1-rho)*U(Xt, X_last, self.ft, self.Dt)\n",
        "        if random:\n",
        "            self.ft = (tf.eye(S) - phi) @ tf.reshape(self.ft, [2, 1]) + tf.reshape(\n",
        "                tfp.distributions.MultivariateNormalFullCovariance(loc=tf.Variable([0.0, 0.0]),\n",
        "                                                                   covariance_matrix=omega).sample(), [2, 1])\n",
        "        else:\n",
        "            self.ft = (tf.eye(S) - phi) @ self.ft\n",
        "        self.Dt = (1 - R) * (self.Dt + C @ tf.reshape((Xt-X_last),[2,1]))\n",
        "        self.t += 1\n",
        "        return self.X_last, self.ft, self.Dt, TR\n",
        "\n",
        "\n",
        "class Critic(keras.Model):\n",
        "    '''\n",
        "    Only consider SGD cases now. Batch with memory buffer will be considerd in future.\n",
        "    In dim: N*H  (St)\n",
        "    Out dim: scalar (value function)\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.L1 = tf.keras.layers.Dense(20, input_shape=(2*S+K,1), activation=None)\n",
        "        self.A1 = tf.keras.layers.ReLU()\n",
        "        self.L2 = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def forward(self, X_last, ft, Dt):\n",
        "        #ft = tf.reshape(ft,[-1])\n",
        "\n",
        "        x = tf.concat([X_last,ft, Dt], 0)\n",
        "        y = self.L2(self.A1(self.L1(tf.transpose(x))))\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "class Actor(keras.Model):\n",
        "    '''\n",
        "    Only consider SGD cases now. Batch with memory buffer will be considerd in future.\n",
        "    In dim: N*H  (St)\n",
        "    Out dim: N*1 (mu)\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Actor, self).__init__()\n",
        "        self.L1 = tf.keras.layers.Dense(S, input_shape=(2*S+K,1), activation=None)\n",
        "        # self.A1 = tf.keras.layers.ReLU()\n",
        "        # self.L2 = tf.keras.layers.Dense(S, input_shape=(100,), activation=None)\n",
        "\n",
        "    def forward(self, X_last, ft, Dt):\n",
        "        #ft = tf.reshape(ft, [-1])\n",
        "\n",
        "        x=tf.concat([X_last,ft, Dt],0)\n",
        "        y = self.L1(tf.transpose(x))\n",
        "        return tf.transpose(y)\n",
        "\n",
        "        # return self.L2(self.A1(self.L1(torch.cat((Xt_1, ft, Dt)))))\n",
        "\n",
        "class ACE:\n",
        "\n",
        "    def __init__(self, new_env=stockEnv()):\n",
        "        self.env = new_env\n",
        "\n",
        "        self.actor_lambda = LAMBDA_actor\n",
        "        self.actor_eligibilities = []\n",
        "\n",
        "        self.critic_lambda = LAMBDA_critic\n",
        "        self.critic_eligibilities = []\n",
        "        self.needreset = True\n",
        "\n",
        "        self.env.reset()\n",
        "\n",
        "    def update_grads_with_eligibility(self, is_critic, delta,grads, P, need_reset):\n",
        "    # Important tip: all list operations will pass by ref.\n",
        "    # So changes to eligibilities will also affect self.critic_eligibilities/self.actor_eligibilities\n",
        "        if is_critic:\n",
        "            params = list(self.critic.trainable_variables)\n",
        "            lamb = self.critic_lambda\n",
        "            eligibilities = self.critic_eligibilities\n",
        "            P = 1\n",
        "        else:\n",
        "            params = list(self.actor.trainable_variables)\n",
        "            lamb = self.actor_lambda\n",
        "            eligibilities = self.actor_eligibilities\n",
        "\n",
        "\n",
        "    # Reset and populate by zeros\n",
        "\n",
        "        if self.needreset:\n",
        "            eligibilities.clear()\n",
        "            for i, p in enumerate(grads):\n",
        "                eligibilities.append(tf.zeros_like(grads[i]))\n",
        "    # update eligibility traces and nn.module weight grad\n",
        "        for i, p in enumerate(grads):\n",
        "\n",
        "             eligibilities[i] = ((1-rho) * lamb * eligibilities[i]) + (P * grads[i])\n",
        "             grads[i] = tf.squeeze(delta) * eligibilities[i]\n",
        "        return grads\n",
        "\n",
        "    def step(self, copied_actor, copied_critic, Xt_sigma_base, Xt_sigma_scale):\n",
        "        self.actor = copied_actor\n",
        "        self.critic = copied_critic\n",
        "\n",
        "        # Action and current value\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(self.critic.trainable_variables)\n",
        "            tape.watch(self.actor.trainable_variables)\n",
        "\n",
        "        Xt_mu = self.actor.forward(self.env.X_last, self.env.ft, self.env.Dt)  # Xt_mu\n",
        "        value = self.critic.forward(self.env.X_last, self.env.ft, self.env.Dt)\n",
        "        # value_pred = torch.cat((value_pred, value.reshape(1,1)), dim=0)\n",
        "        Xt_sigma = Xt_sigma_base + Xt_sigma_scale * np.mean(np.absolute(\n",
        "              tf.stop_gradient(Xt_mu)))\n",
        "        # print(Xt_sigma)\n",
        "        Xt = tf.stop_gradient(tf.random.normal([1], Xt_mu, Xt_sigma, tf.float32, seed=1))\n",
        "        log_prob = tfp.distributions.MultivariateNormalFullCovariance(loc=tf.reshape(Xt_mu,[2,]),covariance_matrix=Xt_sigma ** 2 * tf.eye(Xt_mu.shape[0])).log_prob(tf.reshape(Xt,[2,]))\n",
        "        X_last = self.env.X_last\n",
        "        ft = self.env.ft\n",
        "        Dt = self.env.Dt\n",
        "\n",
        "        if np.mean(np.absolute(tf.stop_gradient(Xt_mu))) >20:\n",
        "            print('stop')\n",
        "\n",
        "        # Reward, state transition, and TD error\n",
        "        _, _, _, TR = self.env.step(Xt)\n",
        "        value_next = self.critic.forward(self.env.X_last, self.env.ft, self.env.Dt)\n",
        "        delta = (TR + (1 - rho) * tf.stop_gradient(value_next) - tf.stop_gradient(value))\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "          l = -value\n",
        "\n",
        "          grads = tape.gradient(l,self.critic.trainable_variables)\n",
        "          critic_grads = self.update_grads_with_eligibility(is_critic=True, delta=delta, P=P, grads=grads, need_reset=self.needreset)\n",
        "        with tf.GradientTape() as tape:\n",
        "          l = -log_prob\n",
        "\n",
        "          grads = tape.gradient(l, self.actor.trainable_variables)\n",
        "          actor_grads = self.update_grads_with_eligibility(is_critic=False, delta=delta, P=P,grads=grads, need_reset=self.needreset)\n",
        "          self.needreset = False\n",
        "\n",
        "        return value, log_prob, TR, delta, critic_grads, actor_grads, Xt, Xt_mu, X_last, ft, Dt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q_8TFI0Drzck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalHead:\n",
        "\n",
        "    def __init__(self, worker_id, random_seed):\n",
        "        self.global_actor = Actor()\n",
        "        self.global_critic = Critic()\n",
        "        self.global_actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_actor)\n",
        "        self.global_critic_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_critic)\n",
        "        self.reward_history = []\n",
        "        self.reward_movavg = []\n",
        "        self.delta_history = []\n",
        "        self.delta_movavg = []\n",
        "        self.worker_id = worker_id\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "    def train(self):\n",
        "        with tf.GradientTape(persistent = True) as tape:\n",
        "            tape.watch(self.global_critic.trainable_variables)\n",
        "            tape.watch(self.global_actor.trainable_variables)\n",
        "            t0 = time.time()\n",
        "            tf.random.set_seed(self.random_seed)\n",
        "            self.ACEs = [ACE(new_env=stockEnv()) for i in range(BATCH_SIZE)]\n",
        "            w_U_env = deepcopy(self.ACEs[-1].env)\n",
        "            iteration = 0\n",
        "\n",
        "            while self.ACEs[0].env.t <= MAX_ITER:\n",
        "                iteration += 1\n",
        "                critic_grads_list = []\n",
        "                actor_grads_list = []\n",
        "                TR_list = []\n",
        "                delta_list = []\n",
        "\n",
        "                # Batches\n",
        "                for ACE_i in self.ACEs:\n",
        "                    value, log_prob, TR, delta, critic_grads, actor_grads, Xt, Xt_mu, X_last, ft, Dt = ACE_i.step(\n",
        "                        copied_actor=deepcopy(self.global_actor), copied_critic=deepcopy(self.global_critic),\n",
        "                        Xt_sigma_base=Xt_sigma_base, Xt_sigma_scale=Xt_sigma_scale)\n",
        "                    critic_grads_list.append(critic_grads)\n",
        "                    actor_grads_list.append(actor_grads)\n",
        "                    TR_list.append(TR)\n",
        "                    delta_list.append(delta)\n",
        "\n",
        "                # update global critic\n",
        "                c = tf.math.reduce_sum(self.global_critic.forward(w_U_env.X_last, w_U_env.ft, w_U_env.Dt)) * 0\n",
        "                global_critic_grads = tape.gradient(c,self.global_critic.trainable_variables)\n",
        "                for i, p in enumerate(list(global_critic_grads)):\n",
        "                        #if not p.requires_grad:\n",
        "                        # continue\n",
        "                    for c_i in critic_grads_list:\n",
        "                        global_critic_grads[i] += (c_i[i] / BATCH_SIZE)\n",
        "                self.global_critic_optimizer.apply_gradients(zip(global_critic_grads,self.global_critic.trainable_variables))\n",
        "\n",
        "                # update global actor\n",
        "                c = tf.math.reduce_sum(self.global_actor.forward(w_U_env.X_last, w_U_env.ft, w_U_env.Dt)) * 0\n",
        "                global_actor_grads = tape.gradient(c, self.global_actor.trainable_variables)\n",
        "                for i, p in enumerate(list(global_actor_grads)):\n",
        "                        #if not p.requires_grad:\n",
        "                            #continue\n",
        "                    for c_i in actor_grads_list:\n",
        "                        global_actor_grads[i] += (c_i[i]/ BATCH_SIZE)\n",
        "                self.global_actor_optimizer.apply_gradients(zip(global_actor_grads,self.global_actor.trainable_variables))\n",
        "\n",
        "                # store diagnostic output (means within the batch)\n",
        "                self.reward_history.append(np.mean(TR_list))\n",
        "                self.delta_history.append(np.mean(delta_list))\n",
        "\n",
        "                # Compute moving averages for printing\n",
        "                window_start = max(iteration - movavg_window + 1, 0)\n",
        "                self.reward_movavg.append(np.array(self.reward_history[window_start:]).mean())\n",
        "                self.delta_movavg.append(np.array(self.delta_history[window_start:]).mean())\n",
        "\n",
        "                # Diagnostic output\n",
        "                if iteration % print_window == 0 and verbose:\n",
        "                    time_elapsed = (time.time() - t0) / 60\n",
        "                    print(f'Worker {self.worker_id} - Timestep: {iteration}')\n",
        "                    print(f'Time elapsed: {time_elapsed:.3f} minutes')\n",
        "                    print('Moving Avg Reward: ', self.reward_movavg[-1])\n",
        "                    print('Moving Avg Delta: ', self.delta_movavg[-1])\n",
        "                    if verbose == 2:\n",
        "                        print('---')\n",
        "                        print('X_last: ', X_last)\n",
        "                        print('ft: ', ft)\n",
        "                        print('Dt: ', Dt)\n",
        "                        print('Action Xt: ', Xt)\n",
        "                        print('Mu Xt: ', Xt_mu)\n",
        "                        print('##############################################################')\n",
        "\n",
        "\n",
        "def TC(Xt, X_last):\n",
        "    deltaX = Xt - X_last\n",
        "    return -0.5 * tf.transpose(deltaX) @ A @ deltaX + tf.transpose(X_last) @ C @ deltaX + 0.5 * tf.transpose(\n",
        "       deltaX) @ C @ deltaX\n",
        "\n",
        "\n",
        "def U(Xt, X_last, ft, Dt):\n",
        "    deltaX = Xt - X_last\n",
        "    return tf.transpose(Xt) @ (B @ ft - (R + rf) * (Dt + C @ deltaX)) - gamma / 2 * tf.transpose(Xt) @ sigma @ Xt\n",
        "\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == tf.keras.Model:\n",
        "        tf.keras.initializers.HeUniform(m.weight)\n",
        "        m.bias.data.fill_(0.00)\n",
        "\n",
        "\n",
        "def simulate(models):\n",
        "    # Plot training statistics for model average\n",
        "    reward_movavg = np.mean([model.reward_movavg for model in models], axis=0)\n",
        "    delta_movavg = np.mean([model.delta_movavg for model in models], axis=0)\n",
        "    plt.figure(figsize=(8, 12))\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.title(\"Reward (Moving Avg)\")\n",
        "    plt.plot(reward_movavg[window_startfrom:])\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.title(\"Delta (Moving Avg)\")\n",
        "    plt.plot(delta_movavg[window_startfrom:])\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.title(\"Delta Squared (Moving Avg)\")\n",
        "    plt.plot(np.array(delta_movavg[window_startfrom:]) ** 2)\n",
        "    plt.savefig(\"./plots/Reward_Delta_\" + str(datetime.datetime.now()).replace(':', '-') + \".png\")\n",
        "    # Simulate model average\n",
        "    env = stockEnv()\n",
        "    env.reset()\n",
        "    Mt1s = []\n",
        "    Xt1s = []\n",
        "    Mt2s = []\n",
        "    Xt2s = []\n",
        "    while env.t <= N_simulations:\n",
        "        Xt_all = [model.global_actor.forward(env.X_last, env.ft, env.Dt).T for model in models]\n",
        "        Xt = tf.stack(Xt_all).mean(axis=0)\n",
        "        Mt = (gamma*sigma).inverse() @ (B @ env.ft)\n",
        "        Mt1s.append(Mt[0])\n",
        "        Xt1s.append(Xt[0].item())\n",
        "        Mt2s.append(Mt[1])\n",
        "        Xt2s.append(Xt[1].item())\n",
        "        env.step(Xt, random=False)\n",
        "    Mt1s = np.array(Mt1s)\n",
        "    Mt2s = np.array(Mt2s)\n",
        "    Xt1s = np.array(Xt1s)\n",
        "    Xt2s = np.array(Xt2s)\n",
        "    # Plot simulation\n",
        "    plt.figure()\n",
        "    plt.quiver(Mt1s[:-1], Mt2s[:-1], Mt1s[1:] - Mt1s[:-1], Mt2s[1:] - Mt2s[:-1], scale_units='xy', angles='xy',\n",
        "               scale=1, label='Markowitz_t')\n",
        "    plt.quiver(Xt1s[:-1], Xt2s[:-1], Xt1s[1:] - Xt1s[:-1], Xt2s[1:] - Xt2s[:-1], scale_units='xy', angles='xy',\n",
        "               scale=1, label='X_t', color='r')\n",
        "    plt.legend()\n",
        "    plt.savefig('./plots/sim_' + str(datetime.datetime.now()).replace(':', '-') + \".png\")\n",
        "\n",
        "\n",
        "def train_model(model):\n",
        "    model.train()\n",
        "    return model\n",
        "\n",
        "\n",
        "def save(models):\n",
        "    with open('/content/drive/My Drive/Plots/' + str(datetime.datetime.now()).replace(':', '-') + \".pkl\", 'wb') as pickle_file:\n",
        "        pickle.dump(models, pickle_file)\n",
        "\n",
        "\n",
        "def load(filename):\n",
        "    with open('/content/drive/My Drive/Plots/' + filename, 'rb') as pickle_file:\n",
        "        return pickle.load(models, pickle_file)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    t0 = time.time()\n",
        "    if not load_model:\n",
        "        models = [GlobalHead(x, tf.random.uniform([1],0,1000000000000000000)) for x in range(N_models)]  # assign id number and random seed to each worker\n",
        "        dask_collect = []\n",
        "        for model in models:\n",
        "            dask_collect.append(delayed(train_model)(model))\n",
        "        if dask_scheduler in ['single-threaded', 'processes', 'threads']:\n",
        "            result, = dask.compute(dask_collect, scheduler=dask_scheduler, n_workers=N_workers)\n",
        "        else:\n",
        "            result, = dask.compute(dask_collect)\n",
        "       # save(result)\n",
        "    else:\n",
        "        result = load(load_model)\n",
        "    simulate(result)\n",
        "    print(f'FINISHED! Total time taken: {time.time() - t0:.3f} seconds')"
      ],
      "metadata": {
        "id": "LmovVhFWr74M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}